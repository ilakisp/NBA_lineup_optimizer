{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Imports, constants and constants' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import pulp\n",
    "import tqdm\n",
    "import datetime\n",
    "import pathlib\n",
    "import importlib\n",
    "\n",
    "sys.path.append('') # # Define your own folder\n",
    "import Send_email # my script\n",
    "import Results_analysis # my script\n",
    "\n",
    "from nba_api.stats.endpoints import leaguegamelog\n",
    "from nba_api.stats.endpoints import boxscoretraditionalv2\n",
    "from nba_api.stats.endpoints import boxscoreadvancedv2\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, BayesianRidge, ElasticNet, RidgeCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "os.chdir('') # Define your own folder\n",
    "connection = sqlite3.connect('') # Define your own folder\n",
    "date_extension = str(datetime.date.today())\n",
    "review_dictionary = {} # initialize dictionary to collect relevant statistics for a final report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Constants and basic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:  \n",
    "    AVAILABLE_SEASONS = ['2016-17', '2017-18', '2018-19', '2019-20']\n",
    "    \n",
    "    BASIC_BOXSCORE_ITEMS = ['GAME_ID', 'TEAM_ID','TEAM_ABBREVIATION','PLAYER_ID', 'MIN', 'PTS',\n",
    "                        'REB', 'AST', 'STL', 'BLK', 'TO', 'FGM', 'FGA', 'FG_PCT']\n",
    "    \n",
    "    BASIC_STATS = ['PTS', 'REB', 'AST', 'STL', 'BLK', 'TO', 'FGM', 'FGA', 'FG_PCT', 'DF_PTS']\n",
    "    \n",
    "    ADVANCED_STATS = ['AST_RATIO','AST_TOV','DEF_RATING', 'DREB_PCT', 'EFG_PCT', 'E_DEF_RATING', 'E_NET_RATING',\n",
    "                     'E_OFF_RATING', 'E_PACE', 'E_TM_TOV_PCT', 'NET_RATING', 'OFF_RATING',\n",
    "                    'OREB_PCT', 'PACE', 'PIE', 'REB_PCT', 'TM_TOV_PCT', 'TS_PCT']\n",
    "    \n",
    "    POSITIONAL_LIMITS = {'PG' : 3, 'SG': 3, 'SF': 3, 'PF': 3, 'C':3, 'TOTAL':7}\n",
    "        \n",
    "lag1, lag2, lag3 = 1, 5, 15 # Lags to be used for rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_points(df): # Define formula to be used for optimization goal\n",
    "    DF_PTS = df.PTS + 1.25 * df.REB + 1.5 * df.AST + 2 * df.STL + 2 * df.BLK - 0.5 * df.TO\n",
    "    return DF_PTS\n",
    "\n",
    "\n",
    "def player_list(): # Create a list of nba players with their IDs\n",
    "    players2019 = pd.read_sql_query(\"SELECT * FROM Player_list\", connection).drop(\n",
    "        columns = ['PLAYER_POSITIONS'], axis = 1).reset_index(drop=True)\n",
    "    if datetime.date.today().day == 1 and input('Do you want to update player list? 1/0 ') == 1:\n",
    "        players2019 = players.get_active_players()\n",
    "        players2019 = pd.DataFrame(players2019).drop(columns='is_active')     \n",
    "        players2019.to_sql('Player_list', connection, if_exists='replace')    \n",
    "    review_dictionary.update({'total number of players': players2019.shape[0]})\n",
    "    return players2019\n",
    "\n",
    "\n",
    "def get_games20162020(): # Get the list of games played over last seasons   \n",
    "    games = leaguegamelog.LeagueGameLog(season='2019-20')\n",
    "    games = games.league_game_log.get_data_frame().loc[:,['GAME_ID', 'GAME_DATE', 'TEAM_ID', 'MATCHUP']]\n",
    "    unique_games = games.drop_duplicates(subset = 'GAME_ID')  \n",
    "    return unique_games\n",
    "\n",
    "\n",
    "def get_game_dates(): # Obtain game dates and the use it to insert in case missing\n",
    "    game_dates = leaguegamelog.LeagueGameLog(season='2019-20')\n",
    "    game_dates = game_dates.league_game_log.get_data_frame().loc[:,['GAME_ID', 'GAME_DATE']]\n",
    "    game_dates = game_dates.drop_duplicates(subset = ['GAME_ID'])\n",
    "    game_dates['GAME_ID'] = game_dates['GAME_ID'].astype(int)\n",
    "    return game_dates\n",
    "    \n",
    "    \n",
    "def switching_columns(lag1, lag2, lag3): # From advanced stats make the list of columns of rolling averages\n",
    "    switching_columns_1 = []\n",
    "    switching_columns_2 = []\n",
    "    for stat in Constants.ADVANCED_STATS:\n",
    "        for lag in [lag1, lag2, lag3]:\n",
    "            item = ['Last{x}_{y}'.format(x=lag, y=stat)]\n",
    "            switching_columns_1.append(item)\n",
    "    switching_columns_2 = copy.deepcopy(switching_columns_1)\n",
    "    switching_columns_1.append(['GAME_ID', 'TEAM_ID'])\n",
    "    # Flatten nested lists (lists of lists) that were created, can't iterate if not flat.\n",
    "    switching_columns_1 = [item for sublist in switching_columns_1 for item in sublist]\n",
    "    switching_columns_2 = [item for sublist in switching_columns_2 for item in sublist]\n",
    "    return switching_columns_1, switching_columns_2\n",
    "\n",
    "\n",
    "def rolling_average(df, window, shift): # Function to calculate rolling averages\n",
    "    return df.rolling(min_periods=window, window=window).mean().shift(shift)\n",
    "\n",
    "\n",
    "def drop_y(df): # After merging two dataframes, unnecessasry columns with _y are added \n",
    "    to_drop = [x for x in df if x.endswith('_y')]\n",
    "    df.drop(columns = to_drop, axis = 1, inplace = True)\n",
    "    for col in df:\n",
    "        if col.endswith('_x'):\n",
    "            df.rename(columns = {col:col.rstrip('_x')}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download basic and advanced boxscores from scratch or update with missing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_boxscores(GamesIDs, seconds): # Download basic boxscores for chosen IDs\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    BoxScore_df = pd.DataFrame()\n",
    "    for ID in tqdm.tqdm(GamesIDs['GAME_ID']):\n",
    "        time.sleep(seconds)\n",
    "        BoxScore = boxscoretraditionalv2.BoxScoreTraditionalV2(game_id=ID)\n",
    "        BoxScore = BoxScore.player_stats.get_data_frame()\n",
    "        BoxScore = BoxScore.loc[:, Constants.BASIC_BOXSCORE_ITEMS] # filters only the chosen boxscore items.\n",
    "        BoxScore['DF_PTS'] = df_points(BoxScore) # Function calculates daily fantasy points.\n",
    "        BoxScore_df = BoxScore_df.append(BoxScore) \n",
    "        BoxScore_df = BoxScore_df.drop_duplicates(subset=['GAME_ID','PLAYER_ID'], keep='first')\n",
    "    BoxScore_df = BoxScore_df.replace(\"\", np.nan, regex=True)\n",
    "        \n",
    "    BoxScore_df['GAME_ID'] = BoxScore_df['GAME_ID'].astype(int)\n",
    "    BoxScore_df = pd.merge(BoxScore_df, game_dates, on='GAME_ID', how='left')\n",
    "    BoxScore_df = BoxScore_df.reset_index().drop_duplicates(subset=['GAME_ID','PLAYER_ID'],keep='first')\n",
    "    BoxScore_df['GAME_DATE'] =  pd.to_datetime(BoxScore_df['GAME_DATE'], errors='coerce')\n",
    "\n",
    "    schedule_input = pd.read_excel('')[['Home_ID', 'Date']] # Define your own folder\n",
    "    schedule_input['Last_HOME'] = 1\n",
    "    BoxScore_df = pd.merge(BoxScore_df, schedule_input, left_on=['TEAM_ID', 'GAME_DATE'], right_on=['Home_ID', 'Date'], \n",
    "                           how='left').drop(['Home_ID', 'Date'], 1) \n",
    "    BoxScore_df['Last_HOME'] = BoxScore_df['Last_HOME'].replace(np.nan, 0, regex=True)\n",
    "    return BoxScore_df\n",
    "\n",
    "\n",
    "def advanced_boxscores(IDs, seconds): # Download advanced boxscores for chosen IDs\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    Advanced_BoxScore_df = pd.DataFrame()\n",
    "    for ID in tqdm.tqdm(IDs['GAME_ID']):\n",
    "        time.sleep(seconds)\n",
    "        BoxScore = boxscoreadvancedv2.BoxScoreAdvancedV2(game_id=ID)\n",
    "        BoxScore = BoxScore.team_stats.get_data_frame()\n",
    "        Advanced_BoxScore_df = Advanced_BoxScore_df.append(BoxScore)\n",
    "        \n",
    "    Advanced_BoxScore_df['GAME_ID'] = Advanced_BoxScore_df['GAME_ID'].astype(int)\n",
    "    Advanced_BoxScore_df = pd.merge(Advanced_BoxScore_df, game_dates, on='GAME_ID', how='left')\n",
    "    Advanced_BoxScore_df = Advanced_BoxScore_df.drop_duplicates()\n",
    "    Advanced_BoxScore_df['GAME_DATE'] =  pd.to_datetime(Advanced_BoxScore_df['GAME_DATE'], errors='coerce')\n",
    "    Advanced_BoxScore_df = Advanced_BoxScore_df.sort_values(by = ['TEAM_ID', 'GAME_DATE']) # sort values for rolling AVG   \n",
    "    Advanced_BoxScore_df = Advanced_BoxScore_df.replace(\"\", np.nan, regex=True) \n",
    "    Advanced_BoxScore_df.to_csv('advanced_boxscore_temp.csv')\n",
    "    return Advanced_BoxScore_df\n",
    "\n",
    "\n",
    "def missing_bscores():   \n",
    "    games = get_games20162020() # run function defined earlier to get the list of all played games\n",
    "    games = games.drop_duplicates('GAME_ID')\n",
    "    gameids = games['GAME_ID']\n",
    "    df1 = pd.DataFrame(data = gameids)\n",
    "    df1.reset_index(drop=True)\n",
    "    df1.index = pd.RangeIndex(len(df1.index))\n",
    "\n",
    "    df2 = pd.read_sql_query(\"SELECT * FROM Appended_advanced\", connection)['GAME_ID'].unique()\n",
    "    df2 = pd.DataFrame(data = df2, columns = ['GAME_ID'])\n",
    "    df2['GAME_ID'] = '00' + df2['GAME_ID'].astype(str)\n",
    "    df3 = pd.concat([df1,df2])\n",
    "    missing_advanced = df3.drop_duplicates(keep = False)\n",
    "\n",
    "    df4 = pd.read_sql_query(\"SELECT * FROM Appended_basic\", connection)['GAME_ID'].unique()\n",
    "    df4 = pd.DataFrame(data = df4, columns = ['GAME_ID'])\n",
    "    df4['GAME_ID'] = '00' + df4['GAME_ID'].astype(str)\n",
    "    df5 = pd.concat([df1, df4])\n",
    "    missing_basic = df5.drop_duplicates(keep = False)\n",
    "    return missing_basic, missing_advanced\n",
    "\n",
    "\n",
    "def update_bscores(missing_basic, missing_advanced):\n",
    "    temp_basic_boxscores = basic_boxscores(missing_basic, 35)\n",
    "    temp_basic_boxscores = temp_basic_boxscores.replace(\"\", np.nan, regex=True)\n",
    "    temp_basic_boxscores.to_sql('Appended_basic', connection, if_exists='append', index=False)\n",
    "      \n",
    "    temp_advanced_boxscores = advanced_boxscores(missing_advanced, 35)\n",
    "    temp_advanced_boxscores = temp_advanced_boxscores.replace(\"\", np.nan, regex=True)    \n",
    "    temp_advanced_boxscores.to_sql('Appended_advanced', connection, if_exists='append')\n",
    "\n",
    "    \n",
    "def call_missing_bscores_update():\n",
    "    # Look up missing boxscores of new games and insert them into the dataframe\n",
    "    missing_basic, missing_advanced = missing_bscores()\n",
    "    print ('missing basic bscores: {}'.format(missing_basic))\n",
    "    if (len(missing_basic) > 0 or len(missing_advanced) > 0):\n",
    "        update_bscores(missing_basic, missing_advanced)\n",
    "    review_dictionary.update({'Added missing basic': len(missing_basic)})\n",
    "    review_dictionary.update({'Added missing advanced': len(missing_basic)})\n",
    "    print ('Updated {} missing basic and {} missing advanced boxscores'.format(len(missing_basic), len(missing_basic)))\n",
    "    \n",
    "    \n",
    "def control_lineup_result():  \n",
    "    # Calculate control lineup actual DF result\n",
    "    control_lineup = pd.read_sql_query(\"SELECT * FROM Control_lineup\", connection)\n",
    "    basic_boxscore = pd.read_sql_query(\"SELECT * FROM Appended_basic\", connection)\n",
    "    basic_boxscore = basic_boxscore.sort_values('GAME_DATE').drop_duplicates('PLAYER_ID', keep='last')\n",
    "    control_lineup_results = (\n",
    "        pd.merge(control_lineup, basic_boxscore, how='left', left_on='PLAYER_ID', right_on='PLAYER_ID'))\n",
    "    output_lineup_sum = control_lineup_results['DF_PTS'].sum()\n",
    "    return output_lineup_sum\n",
    "\n",
    "\n",
    "def review():\n",
    "    review_dictionary.update({'Date': date_extension})\n",
    "    review_dictionary_df = pd.DataFrame.from_dict(review_dictionary, orient='index')\n",
    "    try:\n",
    "        review_dictionary_df.to_sql('review_data', connection, if_exists = 'replace', index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # df has to be in row when adding to sql, but column for sending the email\n",
    "    review_dictionary_df = review_dictionary_df.transpose().round(1) \n",
    "    return review_dictionary_df\n",
    "\n",
    "\n",
    "def new_season_cleanup(): # Tables that should be empty with the start of the new season\n",
    "    user_input = input('Are you sure you want to clean all the SQL tables Y/N: ')\n",
    "    if user_input == 'Y':\n",
    "        db_tables = ['Appended_basic', 'Appended_advanced', 'review_data', 'Control_lineup', 'Combined_boxscores', \n",
    "                    'modelling_data', 'Prediction_calc_data', 'Predictions_data', 'Purchase_data', 'Fanteam_results',\n",
    "                    'Player_positions', 'Predictions_df', 'Progress']\n",
    "        c = connection.cursor() \n",
    "        for table in db_tables:\n",
    "            c.execute(\"DELETE FROM {};\".format(table))\n",
    "        connection.commit()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling and preparation for learning part\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average_df(lag1, lag2, lag3): # Data preparation for modelling/learning\n",
    "    BoxScore_basic_df = pd.read_sql_query(\"SELECT * FROM Appended_basic\", connection)\n",
    "    BoxScore_basic_df = BoxScore_basic_df.replace(\"\", np.nan, regex=True)\n",
    "    BoxScore_basic_df['GAME_DATE'] = pd.to_datetime(BoxScore_basic_df['GAME_DATE'], errors='coerce')\n",
    "    BoxScore_basic_df = BoxScore_basic_df.sort_values(by = ['PLAYER_ID', 'GAME_DATE']) # sort values for rolling averages\n",
    "    BoxScore_basic_df = BoxScore_basic_df.reset_index(drop=True)\n",
    "    # Fill in some data that is missing\n",
    "    BoxScore_basic_df = BoxScore_basic_df.groupby(['PLAYER_ID'], as_index=False).fillna(method = 'ffill', limit = 2)\n",
    "    review_dictionary.update({'Total # of basic BS games:': BoxScore_basic_df['GAME_ID'].nunique()})\n",
    "\n",
    "    # This is to be used not for learning, but predictions later (thus is included in the return, rolling not needed)\n",
    "    actual_basic_df = BoxScore_basic_df.copy(deep=True)\n",
    "    \n",
    "    for stat in Constants.BASIC_STATS:\n",
    "        BoxScore_basic_df['Last{lag}_{s}'.format(lag = lag1, s = stat)] = (\n",
    "        BoxScore_basic_df.groupby(['PLAYER_ID'])[stat].apply(lambda x: rolling_average(x,lag1,1)))\n",
    "        BoxScore_basic_df['Last{lag}_{s}'.format(lag = lag2, s = stat)] = (\n",
    "        BoxScore_basic_df.groupby('PLAYER_ID')[stat].apply(lambda x: rolling_average(x,lag2,1)))\n",
    "        BoxScore_basic_df['Last{lag}_{s}'.format(lag = lag3, s = stat)] = (\n",
    "        BoxScore_basic_df.groupby('PLAYER_ID')[stat].apply(lambda x: rolling_average(x,lag3,1)))\n",
    "       \n",
    "    BoxScore_advanced_df = pd.read_sql_query(\"SELECT * FROM Appended_advanced\", connection)\n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.replace(\"\", np.nan, regex=True)\n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.drop_duplicates(subset=['GAME_ID','TEAM_ID'],keep='first')\n",
    "    BoxScore_advanced_df['GAME_DATE'] = pd.to_datetime(BoxScore_advanced_df['GAME_DATE'], errors='coerce')\n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.sort_values(by = ['TEAM_ID', 'GAME_DATE'])\n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.reset_index(drop=True) \n",
    "    \n",
    "    review_dictionary.update({'Total # of advvanced BS games:': BoxScore_advanced_df['GAME_ID'].nunique()})  \n",
    "    # Fill in some data that is missing\n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.groupby(['TEAM_ID'], as_index=False).fillna(method = 'ffill', limit = 2)\n",
    "    \n",
    "    # This is to be used not for learning, but predictions later (thus is included in the return, rolling not needed)\n",
    "    actual_advanced_df = BoxScore_advanced_df.copy()\n",
    "    \n",
    "    for stat in Constants.ADVANCED_STATS:\n",
    "        BoxScore_advanced_df['Last{lag}_{s}'.format(lag = lag1, s = stat)] = (\n",
    "        BoxScore_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag1,1)))\n",
    "        BoxScore_advanced_df['Last{lag}_{s}'.format(lag = lag2, s = stat)] = (\n",
    "        BoxScore_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag2,1)))\n",
    "        BoxScore_advanced_df['Last{lag}_{s}'.format(lag = lag3, s = stat)] = (\n",
    "        BoxScore_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag3,1)))\n",
    "\n",
    "    # Switch game values that are relevant for opposition \n",
    "    BoxScore_advanced_df = BoxScore_advanced_df.sort_values(by = ['GAME_ID', 'TEAM_ID'])\n",
    "    df2 = BoxScore_advanced_df[switching_columns_1].copy().reset_index(drop=True)\n",
    "    df3 = BoxScore_advanced_df[switching_columns_1].copy().reset_index(drop=True)\n",
    "    df2[switching_columns_2] = BoxScore_advanced_df.groupby('GAME_ID')[switching_columns_2].shift(-1)\n",
    "    df3[switching_columns_2] = BoxScore_advanced_df.groupby('GAME_ID')[switching_columns_2].shift(1)\n",
    "    for col in switching_columns_2:\n",
    "        df2[col].fillna(df3[col], inplace=True) # This is to make a join between the two DFs with shifted values\n",
    "    \n",
    "    # In the main DF replace existing 'switching columns' with switched ones. \n",
    "    BoxScore_advanced_df[switching_columns_2] = df2[switching_columns_2]\n",
    "    \n",
    "    combined_boxscores = pd.merge(BoxScore_basic_df, BoxScore_advanced_df, on=['GAME_ID', 'TEAM_ID'], how='left')\n",
    "    combined_boxscores = combined_boxscores.drop_duplicates(subset=['GAME_ID','PLAYER_ID'], keep='first')\n",
    "    drop_y(combined_boxscores)\n",
    "       \n",
    "    \n",
    "    # Add rest days to the combined boxscore dataframe\n",
    "    combined_boxscores['GAME_DATE'] = pd.to_datetime(combined_boxscores['GAME_DATE'], errors='coerce') \n",
    "    combined_boxscores['Last_rest_days'] = combined_boxscores.groupby('PLAYER_ID')['GAME_DATE'].diff()\n",
    "    combined_boxscores['Last_rest_days'] = combined_boxscores['Last_rest_days'] / np.timedelta64(1,'D')\n",
    "    combined_boxscores['Last_rest_days'] = combined_boxscores['Last_rest_days'].replace(\n",
    "        np.nan, 7, regex=True) # with no prior game,rest is capped at 7 days\n",
    "    \n",
    "    combined_boxscores.to_sql('Combined_boxscores', connection, if_exists='replace')\n",
    "    combined_boxscores.to_csv('combined_boxscores_sanity.csv')\n",
    "    \n",
    "    # Define list of columns that will be attributes in learning\n",
    "    learning_attributes = list(combined_boxscores.loc[:, combined_boxscores.columns.str.startswith('Last')].columns)\n",
    "    data_model_df = combined_boxscores.dropna(subset=learning_attributes).reset_index(drop=True)\n",
    "    data_model_df = data_model_df[learning_attributes]\n",
    "    review_dictionary.update({'Total # of attributes': len(learning_attributes)})\n",
    "    data_model_df.to_sql('modelling_data', connection, if_exists='replace')\n",
    "    \n",
    "    return data_model_df, combined_boxscores, actual_basic_df, actual_advanced_df, learning_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training/testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling():\n",
    "    connection_2018 = sqlite3.connect('') # Define your own folder\n",
    "    data_model_df = pd.read_sql_query(\"SELECT * FROM modelling_data\", connection_2018).iloc[:,2:]\n",
    "    \n",
    "    # Define type of estimator used in modelling\n",
    "    estimators = ['RandomForestRegressor']\n",
    "    types = ['train', 'test']\n",
    "    coefficients = ['rmse', 'r2']\n",
    "    \n",
    "    # Create an empty table (estimators vs types)\n",
    "    rmse_names = [x + '_' + y for y in types for x in estimators] \n",
    "    df_rmse = pd.DataFrame([[0.0] * 2 for j in range(len(rmse_names))], index = rmse_names, columns = coefficients)\n",
    "\n",
    "    # Data preparation to put into scikit. Depended variable y and Independent variables, attributes, X\n",
    "    y = data_model_df['DF_PTS']    \n",
    "    X = data_model_df[learning_attributes].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(x_scaled, columns=learning_attributes)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)    \n",
    "    print ('Data shapes. X_train: {}, y_train: {}, X_test: {}, y_test: {}'.format(\n",
    "        X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "    \n",
    "    # Training the model. Currently only with Rand. Forest, but left flexibility for additional types\n",
    "    for i in range(len(estimators)):\n",
    "        est = estimators[i]\n",
    "        if(est == 'RandomForestRegressor'):\n",
    "            n_estimators = [100]\n",
    "            param_grid = {'n_estimators': n_estimators}\n",
    "            rf = GridSearchCV(RandomForestRegressor(max_depth=7), param_grid, cv=5)\n",
    "            rf.fit(X_train, y_train) \n",
    "        \n",
    "    #Calculating statistical significance\n",
    "    train_rmse = np.sqrt(np.mean((y_train - rf.predict(X_train))**2.0 ))\n",
    "    test_rmse = np.sqrt(np.mean((y_test - rf.predict(X_test))**2.0 ))\n",
    "    train_r2  = r2_score(y_train, rf.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, rf.predict(X_test))\n",
    "\n",
    "    # Adding above calculated measures into the created empty table\n",
    "    for val in types:\n",
    "        for coef in coefficients:\n",
    "            df_rmse.loc[estimators[i] + '_' + val, coef] = eval(val + '_' + coef)  \n",
    "    print ('Model performance: {}'.format(df_rmse))   \n",
    "        \n",
    "    # Save the trained model\n",
    "    Nba_model_pickle = '' # Define your own folder\n",
    "    pickle.dump(rf, open(Nba_model_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling and preparation for predictions part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_preparation(lag1, lag2, lag3): # Prepare data for real valuation\n",
    "    for stat in Constants.BASIC_STATS:\n",
    "        actual_basic_df['Last{lag}_{s}'.format(lag = lag1, s = stat)] = (\n",
    "        actual_basic_df.groupby('PLAYER_ID')[stat].apply(lambda x: rolling_average(x,lag1,0)))\n",
    "        actual_basic_df['Last{lag}_{s}'.format(lag = lag2, s = stat)] = (\n",
    "        actual_basic_df.groupby('PLAYER_ID')[stat].apply(lambda x: rolling_average(x,lag2,0)))\n",
    "        actual_basic_df['Last{lag}_{s}'.format(lag = lag3, s = stat)] = (\n",
    "        actual_basic_df.groupby('PLAYER_ID')[stat].apply(lambda x: rolling_average(x,lag3,0)))  \n",
    "    \n",
    "    for stat in Constants.ADVANCED_STATS:\n",
    "        actual_advanced_df['Last{lag}_{s}'.format(lag = lag1, s = stat)] = (\n",
    "        actual_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag1,0)))\n",
    "        actual_advanced_df['Last{lag}_{s}'.format(lag = lag2, s = stat)] = (\n",
    "        actual_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag2,0)))\n",
    "        actual_advanced_df['Last{lag}_{s}'.format(lag = lag3, s = stat)] = (\n",
    "        actual_advanced_df.groupby('TEAM_ID')[stat].apply(lambda x: rolling_average(x,lag3,0)))  \n",
    "    \n",
    "    # Take the tail (1) value of the list, as that is the only one needed for predict (contains all last info)\n",
    "    actual_basic_tail_df = actual_basic_df.sort_values(by=['PLAYER_ID','GAME_DATE']).groupby('PLAYER_ID').tail(1)\n",
    "    actual_advanced_tail_df = actual_advanced_df.sort_values(by=['TEAM_ID','GAME_DATE']).groupby('TEAM_ID').tail(1)  \n",
    "        \n",
    "    # Calculate how long ago was the last game played compared to today\n",
    "    actual_basic_tail_df['GAME_DATE'] = pd.to_datetime(actual_basic_tail_df['GAME_DATE'], errors='coerce')\n",
    "    today = pd.Timestamp(\"today\").strftime(\"%m/%d/%Y\")\n",
    "    today = pd.to_datetime(today, errors='coerce')\n",
    "    actual_basic_tail_df.loc[:,'Last_rest_days'] = (\n",
    "        actual_basic_tail_df.loc[:,'GAME_DATE'] - datetime.datetime.strptime(date_extension, \"%Y-%m-%d\")) * -1      \n",
    "    actual_basic_tail_df['Last_rest_days'] = actual_basic_tail_df['Last_rest_days'] / np.timedelta64(1,'D')\n",
    "    actual_basic_tail_df.to_csv('actual_basic_tail_df_sanity.csv')\n",
    "    \n",
    "    review_dictionary.update({'Average rest time as of today': actual_basic_tail_df['Last_rest_days'].mean()})\n",
    "    \n",
    "    # Take input (scheduled todays' games) and merge them with the data on those teams respectively\n",
    "    schedule_input = pd.read_excel('') # Define your own folder\n",
    "    schedule_input = schedule_input.where(schedule_input['Date'] == date_extension).dropna()\n",
    "    df1 = pd.merge(actual_basic_tail_df, schedule_input, how = 'left',\n",
    "                   left_on = str('TEAM_ID'), right_on = str('Home_ID')).drop('Home_ID',1)\n",
    "    df2 = pd.merge(actual_basic_tail_df, schedule_input, how = 'left',\n",
    "                   left_on = str('TEAM_ID'), right_on = str('Away_ID')).drop('Away_ID',1)\n",
    "    df1['Home_ID'] = df2['Home_ID']\n",
    "    df1['Opponent'] = df1['Away_ID'].fillna(df1['Home_ID'])\n",
    "    df3 = pd.merge(df1, actual_advanced_tail_df[switching_columns_1],\n",
    "                   how= 'left', left_on = 'Opponent', right_on = 'TEAM_ID')\n",
    "    df3[learning_attributes] = df3[learning_attributes].round(2)\n",
    "    drop_y(df3)\n",
    "    df3 = df3.dropna(subset = learning_attributes).reset_index(drop=True)\n",
    "   \n",
    "    # Data processing for model input, the same way as in learning part\n",
    "    df4 = df3[learning_attributes].values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df4 = min_max_scaler.fit_transform(df4)\n",
    "    df4 = pd.DataFrame(df4, columns = learning_attributes)    \n",
    "    df4.to_sql('Prediction_calc_data', connection, if_exists='replace')\n",
    "    \n",
    "    # Call controll lineup performance calculation before the new control lineup is created\n",
    "    review_dictionary.update({'Control_lineup_performance': control_lineup_result()}) \n",
    "    return df4, df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betting-website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Fanteam_data # Web scraping script that I will not share. Please reach out if you would like some advice on this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, fanteam_salaries, predicted_players):\n",
    "    rf = pickle.load(open('', 'rb')) # Define your own folder\n",
    "    predictions = rf.predict(df)\n",
    "    \n",
    "    # Convert predictions array to a dataframe\n",
    "    ind = [str(i) for i in range(1, len(predictions)+1)] # make index for the dataframe being created\n",
    "    predictions_df = pd.DataFrame(predictions, index = ind, columns = ['predictions']).reset_index()\n",
    "    predictions_df['PLAYER_ID'] = predicted_players['PLAYER_ID']\n",
    "    predictions_df.drop(columns = ['index'], inplace = True)\n",
    "    predictions_df = pd.merge(predictions_df, fanteam_salaries, \\\n",
    "             how = 'left', left_on = 'PLAYER_ID', right_on = 'PERSON_ID').drop('PERSON_ID', 1) #Add column with positions\n",
    "    \n",
    "    predictions_df = predictions_df.rename(columns={'SALARIES': 'Salary'})\n",
    "    predictions_df['Salary'] = predictions_df['Salary'].str.replace('M','') # Convert salary string to numbers\n",
    "    predictions_df['Salary'] = pd.to_numeric(predictions_df['Salary'], errors='coerce')*1000000\n",
    "    \n",
    "    #predictions_df['Salary'] = np.random.randint(5000, 15000, predictions_df.shape[0])\n",
    "    predictions_df['Date'] = date_extension\n",
    "    # Adding a column in the predictions dataframe that will contain simple average to be used for control team.\n",
    "    predictions_df = predictions_df.sort_values(by=['PLAYER_ID'])\n",
    "    predicted_players = predicted_players.sort_values(by=['PLAYER_ID'])\n",
    "    \n",
    "    predictions_df = pd.merge(predictions_df, predicted_players[['PLAYER_ID', 'Last5_DF_PTS']], how='left', on='PLAYER_ID')\n",
    "    predictions_df = predictions_df.rename({'Last5_DF_PTS': 'Control_last5'}, axis='columns') # to be changed back to15,5,1\n",
    "\n",
    "    review_dictionary.update({'Total # of predicted players': predictions_df['PLAYER_ID'].shape[0]})\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lineup(predictions_df, budget, players_limit, type='main'):    \n",
    "    # Create a distinction between ML optimization and control/base optimization by selecting column\n",
    "    if type == 'main':\n",
    "        main_variable = 'predictions'\n",
    "    elif type == 'control':\n",
    "        main_variable = 'Control_last5'\n",
    "    else:\n",
    "        raise ValueError('Value entered is not for main or control optimization')\n",
    "    \n",
    "    predictions_df = predictions_df.dropna(subset=['Salary', 'PLAYER_NAMES', 'Control_last5']).reset_index(drop=True)\n",
    "    print ('The number of players used in optimization is: {}'.format(len(predictions_df['Salary'])))\n",
    "    \n",
    "    # Initialize pulp with a optimization objective\n",
    "    problem = pulp.LpProblem('Maximization problem', pulp.LpMaximize)\n",
    "\n",
    "    # Create binary variable (column) for each of the players in the dataset\n",
    "    decision_variables = []\n",
    "    for rownum, row in predictions_df.iterrows():\n",
    "        variable = str('x' + str(rownum))\n",
    "        variable = pulp.LpVariable(str(variable), lowBound = 0, upBound = 1, cat= 'Integer')\n",
    "        decision_variables.append(variable)    \n",
    "\n",
    "    # Create objective function - sum of predicted DF points multiplied by 0/1 - binary player variable\n",
    "    Sum_predictions = \"\"\n",
    "    for rownum, row in predictions_df.iterrows():\n",
    "        for i, binary in enumerate(decision_variables):\n",
    "            if (rownum == i):\n",
    "                Sum_predictions += row[main_variable] * binary\n",
    "    problem += Sum_predictions\n",
    "\n",
    "    # Implementing all constraints\n",
    "    number_of_players = \"\"\n",
    "    total_salary = \"\"\n",
    "    number_of_pg = \"\"\n",
    "    number_of_sg = \"\"\n",
    "    number_of_sf = \"\"\n",
    "    number_of_pf = \"\"\n",
    "    number_of_c  = \"\"\n",
    "\n",
    "    for rownum, row in predictions_df.iterrows():\n",
    "        for i, binary in enumerate(decision_variables):\n",
    "            if (rownum == i):\n",
    "                total_salary += row['Salary'] * binary\n",
    "                number_of_players += binary\n",
    "                if row['PLAYER_POSITIONS'] == 'PG':\n",
    "                    number_of_pg += binary * 1               \n",
    "                if row['PLAYER_POSITIONS'] == 'SG':\n",
    "                    number_of_sg += binary * 1\n",
    "                if row['PLAYER_POSITIONS'] == 'SF':\n",
    "                    number_of_sf += binary * 1    \n",
    "                if row['PLAYER_POSITIONS'] == 'PF':\n",
    "                    number_of_pf += binary * 1    \n",
    "                if row['PLAYER_POSITIONS'] == 'C':\n",
    "                    number_of_c += binary * 1         \n",
    "\n",
    "    # Constraint limits\n",
    "    problem += (total_salary <= budget)\n",
    "    problem += (number_of_players == Constants.POSITIONAL_LIMITS['TOTAL'])\n",
    "    problem += (number_of_pg <= Constants.POSITIONAL_LIMITS['PG'])\n",
    "    problem += (number_of_sg <= Constants.POSITIONAL_LIMITS['SG'])\n",
    "    problem += (number_of_sf <= Constants.POSITIONAL_LIMITS['SF'])\n",
    "    problem += (number_of_pf <= Constants.POSITIONAL_LIMITS['PF'])\n",
    "    problem += (number_of_c  <= Constants.POSITIONAL_LIMITS['C'])\n",
    "    problem += (1 <= number_of_pg)\n",
    "    problem += (1 <= number_of_sg)\n",
    "    problem += (1 <= number_of_sf)\n",
    "    problem += (1 <= number_of_pf)\n",
    "    problem += (1 <= number_of_c)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Run optimization\n",
    "    optimization_result = problem.solve()\n",
    "    \n",
    "    # Present results, using optimization results to create a dataframe with that data\n",
    "    variable_name = []\n",
    "    variable_value = []\n",
    "    for v in problem.variables():\n",
    "        variable_name.append(v.name)\n",
    "        variable_value.append(v.varValue)\n",
    "    output = pd.DataFrame({'Variable': variable_name, 'Value': variable_value})\n",
    "\n",
    "    for rownum, row in output.iterrows():\n",
    "        value = re.findall(r'(\\d+)', row['Variable'])\n",
    "        output.loc[rownum, 'variable'] = int(value[0])\n",
    "        output = output.sort_values(by='variable').reset_index(drop = True)\n",
    "        \n",
    "    # Attaching the output column to the dataframe with original statistics information\n",
    "    for rownum, row in predictions_df.iterrows():\n",
    "        for results_rownum, results_row in output.iterrows():\n",
    "            if (rownum == results_rownum):\n",
    "                predictions_df.loc[rownum, 'Purchase_decision'] = results_row['Value']    \n",
    "\n",
    "    # Attach player names to the decisions and filter out only the purchasing ones                \n",
    "    predictions_df = (pd.merge(predictions_df, players[['PERSON_ID', 'PLAYERCODE']],\n",
    "                    how = 'left', left_on = 'PLAYER_ID', right_on = 'PERSON_ID'))  \n",
    "    predictions_df['Date'] = date_extension\n",
    "    purchase_df = predictions_df.loc[predictions_df['Purchase_decision'] == 1]\n",
    "        \n",
    "    if type == 'main':\n",
    "        predictions_df.to_sql('Predictions_data', connection, if_exists='replace')\n",
    "        purchase_df.to_sql('Purchase_data', connection, if_exists='replace')\n",
    "        review_dictionary.update({'Total # of variales (optimization)': len(decision_variables)})\n",
    "        review_dictionary.update({'Total # of missing salaries': predictions_df['Salary'].isna().sum()})\n",
    "        review_dictionary.update({'Total DF salary': purchase_df['Salary'].sum()})\n",
    "        review_dictionary.update({'Total predicted DF points': purchase_df['predictions'].sum()})\n",
    "    elif type == 'control':\n",
    "        review_dictionary.update({'Total base DF salary': purchase_df['Salary'].sum()})\n",
    "        review_dictionary.update({'Total base predicted DF points': purchase_df['Control_last5'].sum()})\n",
    "        purchase_df['PLAYER_ID'].to_sql('Control_lineup', connection, if_exists='replace')\n",
    "           \n",
    "    print ('{} lineup is ready!'.format('Optimal' if type=='main' else 'Control'))\n",
    "    return purchase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running main module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " new_season_cleanup() \n",
    "\n",
    "# Start a separate thread to run and fetch fanteam data in parallel with getting boxscore data\n",
    "fanteam_returns = dict() # empty dictionary to collect the results (scraped data) of imported fanteam_data function \n",
    "fanteam_thread = threading.Thread(target=Fanteam_data.fanteam_data, args=(fanteam_returns,))\n",
    "fanteam_thread.start() # Thread starts to scrape fanteam data, following rows executed in paralel\n",
    "\n",
    "game_dates = get_game_dates()\n",
    "\n",
    "players = player_list()\n",
    "\n",
    "switching_columns_1, switching_columns_2 = switching_columns(lag1, lag2, lag3)\n",
    "\n",
    "call_missing_bscores_update()\n",
    "\n",
    "data_model_df, data_df, actual_basic_df, actual_advanced_df, learning_attributes = rolling_average_df(lag1, lag2, lag3) # +\n",
    "\n",
    "if raw_input('Would you like to run modelling 1/0?') == 1: modelling()\n",
    "\n",
    "prediction_ready_data, predicted_players = evaluation_preparation(lag1, lag2, lag3) \n",
    "\n",
    "fanteam_thread.join() # join makes the main thread stop and wait for fanteam_thread to finish before continuing main\n",
    "\n",
    "predictions_df = predict(prediction_ready_data, fanteam_returns['salaries_df'], predicted_players)\n",
    "\n",
    "optimal_lineup = optimize_lineup(predictions_df,fanteam_returns['budget'],fanteam_returns['players_limit'], type='main')\n",
    "control_lineup = optimize_lineup(predictions_df,fanteam_returns['budget'],fanteam_returns['players_limit'], type='control')\n",
    "\n",
    "review_dictionary_df = review()\n",
    "\n",
    "Send_email.send_email(\n",
    "    optimal_lineup[['PLAYER_POSITIONS', 'PLAYERCODE', 'Salary']],\n",
    "    control_lineup[['PLAYER_POSITIONS', 'PLAYERCODE', 'Salary']],\n",
    "    review_dictionary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
